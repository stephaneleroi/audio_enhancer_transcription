#!/usr/bin/env python3
"""
Module de diarisation adaptative des locuteurs basÃ© sur pyannote.
Version adaptative respectant les rÃ¨gles .cursorrules : ZÃ‰RO valeur codÃ©e en dur.
Tous les paramÃ¨tres sont calculÃ©s dynamiquement selon les caractÃ©ristiques audio dÃ©tectÃ©es.
"""

import os
import sys
import argparse
import logging
import numpy as np
import librosa
import soundfile as sf
from typing import Any, Tuple, Dict, List, Optional
import time
import psutil
from concurrent.futures import ProcessPoolExecutor, as_completed
import pickle
import tempfile
from dataclasses import dataclass
from pathlib import Path

# Imports pyannote
try:
    from pyannote.audio import Pipeline
    from pyannote.core import Annotation, Segment
    import torch
    PYANNOTE_AVAILABLE = True
except ImportError:
    PYANNOTE_AVAILABLE = False
    print("âš ï¸ pyannote.audio non disponible. Installation requise : pip install pyannote.audio")

@dataclass
class SpeakerSegment:
    """ReprÃ©sente un segment de parole avec locuteur identifiÃ©."""
    start: float
    end: float
    speaker: str
    confidence: float = 0.0
    
    def duration(self) -> float:
        return self.end - self.start
    
    def __str__(self) -> str:
        return f"[{self.start:.1f}s - {self.end:.1f}s] {self.speaker} (conf: {self.confidence:.3f})"

def setup_logging(verbose=False):
    """Configure le systÃ¨me de journalisation."""
    level = logging.DEBUG if verbose else logging.INFO
    
    logger = logging.getLogger()
    logger.setLevel(level)
    
    # Supprimer les handlers existants pour Ã©viter les doublons
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    console_handler = logging.StreamHandler()
    console_handler.setLevel(level)
    
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(formatter)
    
    logger.addHandler(console_handler)
    
    return logger

def analyze_audio_for_diarization(y: np.ndarray, sr: int, logger=None) -> Dict[str, Any]:
    """
    Objectif : Analyse spÃ©cialisÃ©e pour la diarisation des locuteurs
    
    Analyse les caractÃ©ristiques audio pertinentes pour la sÃ©paration des locuteurs :
    - Variations spectrales (changements de voix)
    - Pauses et silences (transitions entre locuteurs)
    - Ã‰nergie et dynamique (intensitÃ© vocale)
    - Estimation du nombre de locuteurs potentiels
    """
    if logger:
        logger.info("ğŸ” Analyse audio spÃ©cialisÃ©e pour diarisation...")
    
    start_time = time.time()
    
    # ParamÃ¨tres adaptatifs pour l'analyse
    frame_length = int(0.025 * sr)  # 25ms
    hop_length = int(0.010 * sr)    # 10ms
    
    # Analyse Ã©nergÃ©tique
    energy = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]
    
    # Analyse spectrale pour dÃ©tecter les changements de voix
    spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=hop_length)[0]
    spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr, hop_length=hop_length)[0]
    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr, hop_length=hop_length)[0]
    
    # MFCC pour caractÃ©riser les voix
    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, hop_length=hop_length)
    
    # Calcul des variations spectrales (indicateur de changement de locuteur)
    spectral_variation = np.std(np.diff(spectral_centroids))
    mfcc_variation = np.mean(np.std(mfccs, axis=1))
    
    # DÃ©tection des silences adaptatifs
    energy_percentiles = np.percentile(energy, [5, 10, 25, 50, 75, 90, 95])
    silence_threshold = energy_percentiles[1]  # percentile 10
    
    # Analyse des pauses (potentiels changements de locuteur)
    silence_frames = energy < silence_threshold
    
    # Calcul des durÃ©es de silence
    silence_durations = []
    in_silence = False
    silence_start = 0
    
    for i, is_silent in enumerate(silence_frames):
        if is_silent and not in_silence:
            silence_start = i
            in_silence = True
        elif not is_silent and in_silence:
            duration_ms = (i - silence_start) * hop_length / sr * 1000
            silence_durations.append(duration_ms)
            in_silence = False
    
    # Estimation du nombre de locuteurs basÃ©e sur les variations
    # Plus il y a de variations spectrales, plus il y a potentiellement de locuteurs
    estimated_speakers = max(1, min(6, int(2 + spectral_variation * 10)))
    
    # Calcul des seuils adaptatifs pour la segmentation
    mean_energy = np.mean(energy)
    energy_std = np.std(energy)
    dynamic_range = np.max(energy) - np.min(energy)
    
    # Estimation de la qualitÃ© pour la diarisation
    snr_estimate = 20 * np.log10(np.mean(energy[energy > silence_threshold]) / 
                                max(np.mean(energy[silence_frames]), 1e-10))
    
    characteristics = {
        'duration': len(y) / sr,
        'sample_rate': sr,
        'mean_energy': mean_energy,
        'energy_std': energy_std,
        'dynamic_range': dynamic_range,
        'energy_percentiles': energy_percentiles,
        'silence_threshold': silence_threshold,
        'spectral_variation': spectral_variation,
        'mfcc_variation': mfcc_variation,
        'estimated_speakers': estimated_speakers,
        'silence_durations': silence_durations,
        'mean_silence_duration': np.mean(silence_durations) if silence_durations else 0,
        'snr_estimate': snr_estimate,
        'spectral_center_mean': np.mean(spectral_centroids),
        'spectral_center_std': np.std(spectral_centroids),
        'recording_quality': "high" if snr_estimate > 20 else "medium" if snr_estimate > 10 else "low"
    }
    
    if logger:
        logger.info(f"ğŸ“Š Analyse terminÃ©e en {time.time() - start_time:.2f}s")
        logger.info(f"   â€¢ DurÃ©e: {characteristics['duration']:.1f}s")
        logger.info(f"   â€¢ Locuteurs estimÃ©s: {estimated_speakers}")
        logger.info(f"   â€¢ Variation spectrale: {spectral_variation:.4f}")
        logger.info(f"   â€¢ Variation MFCC: {mfcc_variation:.4f}")
        logger.info(f"   â€¢ SNR estimÃ©: {snr_estimate:.1f} dB")
        logger.info(f"   â€¢ Silences moyens: {characteristics['mean_silence_duration']:.0f}ms")
    
    return characteristics

def calculate_diarization_parameters(characteristics: Dict[str, Any], logger=None) -> Dict[str, Any]:
    """
    Objectif : Calcul automatique des paramÃ¨tres de diarisation optimaux
    
    GÃ©nÃ¨re tous les paramÃ¨tres de diarisation en se basant uniquement sur les
    caractÃ©ristiques mesurÃ©es du signal audio. Aucune valeur n'est codÃ©e en dur.
    """
    if logger:
        logger.info("ğŸ§® Calcul des paramÃ¨tres de diarisation adaptatifs...")
    
    # Calcul du nombre optimal de locuteurs
    base_speakers = characteristics['estimated_speakers']
    
    # Ajustement selon la qualitÃ© et les variations
    quality_factor = {
        'high': 1.0,
        'medium': 0.8,
        'low': 0.6
    }[characteristics['recording_quality']]
    
    variation_factor = min(2.0, characteristics['spectral_variation'] * 5)
    
    # Nombre de locuteurs adaptatif avec bornes de sÃ©curitÃ©
    min_speakers = max(1, int(base_speakers * quality_factor * 0.7))
    max_speakers = max(2, int(base_speakers * quality_factor * variation_factor))
    max_speakers = min(max_speakers, 8)  # Limite raisonnable
    
    # Seuil de segmentation adaptatif
    # BasÃ© sur les variations spectrales et la qualitÃ©
    base_segmentation_threshold = 0.1 + characteristics['spectral_variation'] * 2.0
    segmentation_threshold = max(0.05, min(0.8, base_segmentation_threshold))
    
    # DurÃ©e minimale de segment adaptative
    # BasÃ©e sur la durÃ©e moyenne des silences
    mean_silence = characteristics['mean_silence_duration']
    if mean_silence > 0:
        min_segment_duration = max(0.5, min(3.0, mean_silence / 1000 * 2))
    else:
        min_segment_duration = 1.0
    
    # Seuil de clustering adaptatif
    # Plus il y a de variations, plus le seuil doit Ãªtre fin
    clustering_threshold = max(0.1, min(0.7, 0.4 - characteristics['mfcc_variation'] * 0.1))
    
    # Taille de chunk adaptative selon la durÃ©e totale
    duration = characteristics['duration']
    if duration < 60:  # < 1 minute
        chunk_duration = min(30, duration / 2)
    elif duration < 300:  # < 5 minutes
        chunk_duration = 60
    else:  # > 5 minutes
        chunk_duration = 120
    
    # Overlap adaptatif
    overlap_duration = max(5, min(15, chunk_duration * 0.1))
    
    parameters = {
        'min_speakers': min_speakers,
        'max_speakers': max_speakers,
        'segmentation_threshold': segmentation_threshold,
        'clustering_threshold': clustering_threshold,
        'min_segment_duration': min_segment_duration,
        'chunk_duration': chunk_duration,
        'overlap_duration': overlap_duration,
        'quality_factor': quality_factor,
        'variation_factor': variation_factor
    }
    
    if logger:
        logger.info("ğŸ“‹ ParamÃ¨tres de diarisation calculÃ©s :")
        logger.info(f"   â€¢ Locuteurs: {min_speakers}-{max_speakers}")
        logger.info(f"   â€¢ Seuil segmentation: {segmentation_threshold:.3f}")
        logger.info(f"   â€¢ Seuil clustering: {clustering_threshold:.3f}")
        logger.info(f"   â€¢ DurÃ©e min segment: {min_segment_duration:.1f}s")
        logger.info(f"   â€¢ Chunks: {chunk_duration:.0f}s (overlap: {overlap_duration:.0f}s)")
    
    return parameters

def create_diarization_pipeline(parameters: Dict[str, Any], logger=None) -> Optional[Pipeline]:
    """
    Objectif : CrÃ©ation du pipeline pyannote avec paramÃ¨tres adaptatifs
    
    Configure le pipeline de diarisation avec les paramÃ¨tres calculÃ©s automatiquement.
    """
    if not PYANNOTE_AVAILABLE:
        if logger:
            logger.error("âŒ pyannote.audio non disponible")
        return None
    
    if logger:
        logger.info("ğŸ”§ CrÃ©ation du pipeline de diarisation adaptatif...")
    
    try:
        # Utilisation du modÃ¨le prÃ©-entraÃ®nÃ©
        pipeline = Pipeline.from_pretrained(
            "pyannote/speaker-diarization-3.1",
            use_auth_token=None  # Peut nÃ©cessiter un token HuggingFace
        )
        
        # Configuration adaptative du pipeline
        if hasattr(pipeline, '_segmentation'):
            # Ajustement des paramÃ¨tres de segmentation
            pipeline._segmentation.min_duration_on = parameters['min_segment_duration']
            pipeline._segmentation.min_duration_off = parameters['min_segment_duration'] * 0.5
        
        if hasattr(pipeline, '_clustering'):
            # Ajustement des paramÃ¨tres de clustering
            pipeline._clustering.threshold = parameters['clustering_threshold']
        
        if logger:
            logger.info("âœ… Pipeline de diarisation configurÃ©")
        
        return pipeline
        
    except Exception as e:
        if logger:
            logger.error(f"âŒ Erreur crÃ©ation pipeline: {str(e)}")
        return None

def process_audio_chunk_diarization(chunk_data: bytes) -> List[SpeakerSegment]:
    """
    Objectif : Traitement d'un chunk audio pour la diarisation
    
    Fonction sÃ©rialisable pour le traitement parallÃ¨le des chunks.
    """
    try:
        # DÃ©sÃ©rialisation des donnÃ©es
        data = pickle.loads(chunk_data)
        audio_file = data['audio_file']
        start_time = data['start_time']
        end_time = data['end_time']
        parameters = data['parameters']
        chunk_id = data['chunk_id']
        
        # Chargement du chunk audio
        y, sr = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time-start_time)
        
        # Sauvegarde temporaire du chunk
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_file:
            sf.write(tmp_file.name, y, sr)
            temp_path = tmp_file.name
        
        try:
            # CrÃ©ation du pipeline pour ce worker
            pipeline = create_diarization_pipeline(parameters)
            if pipeline is None:
                return []
            
            # Application de la diarisation
            diarization = pipeline(temp_path)
            
            # Conversion en segments avec ajustement temporel
            segments = []
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                segment = SpeakerSegment(
                    start=turn.start + start_time,
                    end=turn.end + start_time,
                    speaker=speaker,
                    confidence=1.0  # pyannote ne fournit pas de score de confiance direct
                )
                segments.append(segment)
            
            return segments
            
        finally:
            # Nettoyage du fichier temporaire
            if os.path.exists(temp_path):
                os.unlink(temp_path)
                
    except Exception as e:
        print(f"âŒ Erreur traitement chunk: {str(e)}")
        return []

def diarize_audio_adaptive(audio_file: str, parameters: Dict[str, Any], 
                          num_workers: int = 4, logger=None) -> List[SpeakerSegment]:
    """
    Objectif : Diarisation adaptative avec traitement parallÃ¨le par chunks
    
    Applique la diarisation en dÃ©coupant l'audio en chunks et en traitant
    en parallÃ¨le avec les paramÃ¨tres calculÃ©s automatiquement.
    """
    if logger:
        logger.info("ğŸš€ DÃ©but de la diarisation adaptative parallÃ¨le...")
    
    # Chargement de l'audio pour analyse
    y, sr = librosa.load(audio_file, sr=16000)
    duration = len(y) / sr
    
    chunk_duration = parameters['chunk_duration']
    overlap_duration = parameters['overlap_duration']
    
    # Calcul des chunks avec overlap
    chunks = []
    start_time = 0
    chunk_id = 0
    
    while start_time < duration:
        end_time = min(start_time + chunk_duration, duration)
        
        # PrÃ©paration des donnÃ©es pour le worker
        chunk_data = {
            'audio_file': audio_file,
            'start_time': start_time,
            'end_time': end_time,
            'parameters': parameters,
            'chunk_id': chunk_id
        }
        
        chunks.append(pickle.dumps(chunk_data))
        
        # Avancement avec overlap
        start_time += chunk_duration - overlap_duration
        chunk_id += 1
        
        if end_time >= duration:
            break
    
    if logger:
        logger.info(f"ğŸ“¦ {len(chunks)} chunks crÃ©Ã©s (durÃ©e: {chunk_duration}s, overlap: {overlap_duration}s)")
        logger.info(f"ğŸ–¥ï¸ Traitement parallÃ¨le avec {num_workers} workers")
    
    # Traitement parallÃ¨le
    all_segments = []
    completed_chunks = 0
    
    with ProcessPoolExecutor(max_workers=num_workers) as executor:
        # Soumission des tÃ¢ches
        future_to_chunk = {
            executor.submit(process_audio_chunk_diarization, chunk_data): i 
            for i, chunk_data in enumerate(chunks)
        }
        
        # Collecte des rÃ©sultats
        for future in as_completed(future_to_chunk):
            chunk_idx = future_to_chunk[future]
            try:
                segments = future.result()
                all_segments.extend(segments)
                completed_chunks += 1
                
                if logger:
                    progress = (completed_chunks / len(chunks)) * 100
                    logger.info(f"ğŸ“Š Chunk {chunk_idx+1}/{len(chunks)} terminÃ©: "
                              f"{len(segments)} segments - ProgrÃ¨s: {progress:.1f}%")
                    
            except Exception as e:
                if logger:
                    logger.error(f"âŒ Erreur chunk {chunk_idx}: {str(e)}")
    
    # Post-traitement : fusion des segments overlappants et nettoyage
    merged_segments = merge_overlapping_segments(all_segments, parameters, logger)
    
    if logger:
        logger.info(f"âœ… Diarisation terminÃ©e: {len(merged_segments)} segments finaux")
    
    return merged_segments

def merge_overlapping_segments(segments: List[SpeakerSegment], parameters: Dict[str, Any], 
                             logger=None) -> List[SpeakerSegment]:
    """
    Objectif : Fusion intelligente des segments overlappants
    
    Fusionne les segments provenant de chunks diffÃ©rents en Ã©vitant les doublons
    et en optimisant la continuitÃ© des locuteurs.
    """
    if not segments:
        return []
    
    if logger:
        logger.info(f"ğŸ”§ Fusion de {len(segments)} segments bruts...")
    
    # Tri par temps de dÃ©but
    segments.sort(key=lambda s: s.start)
    
    merged = []
    current_segment = segments[0]
    
    for next_segment in segments[1:]:
        # VÃ©rification de l'overlap
        if (next_segment.start <= current_segment.end and 
            next_segment.speaker == current_segment.speaker):
            # Fusion des segments du mÃªme locuteur qui se chevauchent
            current_segment.end = max(current_segment.end, next_segment.end)
            current_segment.confidence = max(current_segment.confidence, next_segment.confidence)
        else:
            # Ajout du segment actuel et passage au suivant
            if current_segment.duration() >= parameters['min_segment_duration']:
                merged.append(current_segment)
            current_segment = next_segment
    
    # Ajout du dernier segment
    if current_segment.duration() >= parameters['min_segment_duration']:
        merged.append(current_segment)
    
    if logger:
        logger.info(f"âœ… {len(merged)} segments aprÃ¨s fusion")
    
    return merged

def save_diarization_results(segments: List[SpeakerSegment], output_file: str, logger=None):
    """
    Objectif : Sauvegarde des rÃ©sultats de diarisation
    
    Sauvegarde les segments avec locuteurs dans diffÃ©rents formats.
    """
    if logger:
        logger.info(f"ğŸ’¾ Sauvegarde des rÃ©sultats: {output_file}")
    
    # Format texte dÃ©taillÃ©
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("RÃ‰SULTATS DE DIARISATION ADAPTATIVE\n")
        f.write("=" * 50 + "\n\n")
        
        for i, segment in enumerate(segments, 1):
            f.write(f"{i:3d}. {segment}\n")
        
        f.write(f"\nğŸ“Š STATISTIQUES:\n")
        f.write(f"   â€¢ Segments totaux: {len(segments)}\n")
        f.write(f"   â€¢ Locuteurs uniques: {len(set(s.speaker for s in segments))}\n")
        f.write(f"   â€¢ DurÃ©e totale: {max(s.end for s in segments):.1f}s\n")
        
        # Statistiques par locuteur
        speaker_stats = {}
        for segment in segments:
            if segment.speaker not in speaker_stats:
                speaker_stats[segment.speaker] = {'duration': 0, 'segments': 0}
            speaker_stats[segment.speaker]['duration'] += segment.duration()
            speaker_stats[segment.speaker]['segments'] += 1
        
        f.write(f"\nğŸ‘¥ STATISTIQUES PAR LOCUTEUR:\n")
        for speaker, stats in sorted(speaker_stats.items()):
            f.write(f"   â€¢ {speaker}: {stats['duration']:.1f}s ({stats['segments']} segments)\n")
    
    # Format RTTM (Rich Transcription Time Marked)
    rttm_file = output_file.replace('.txt', '.rttm')
    with open(rttm_file, 'w') as f:
        for segment in segments:
            f.write(f"SPEAKER audio 1 {segment.start:.3f} {segment.duration():.3f} "
                   f"<NA> <NA> {segment.speaker} <NA> <NA>\n")
    
    if logger:
        logger.info(f"ğŸ“ Fichiers sauvegardÃ©s: {output_file} et {rttm_file}")

def main():
    parser = argparse.ArgumentParser(
        description="Diarisation adaptative des locuteurs - ZÃ‰RO valeur codÃ©e en dur"
    )
    
    parser.add_argument('input_file', help='Fichier audio Ã  analyser')
    parser.add_argument('--output', '-o', help='Fichier de sortie (optionnel)')
    parser.add_argument('--verbose', '-v', action='store_true', help='Affichage dÃ©taillÃ©')
    parser.add_argument('--workers', '-w', type=int, default=4, 
                       help='Nombre de workers parallÃ¨les (dÃ©faut: 4)')
    
    args = parser.parse_args()
    
    if not PYANNOTE_AVAILABLE:
        print("âŒ pyannote.audio requis. Installation: pip install pyannote.audio")
        sys.exit(1)
    
    logger = setup_logging(args.verbose)
    
    # GÃ©nÃ©ration automatique du nom de sortie
    if not args.output:
        base, ext = os.path.splitext(args.input_file)
        args.output = f"{base}_diarization.txt"
    
    try:
        logger.info("=" * 60)
        logger.info("ğŸ­ DIARISATION ADAPTATIVE DES LOCUTEURS")
        logger.info("=" * 60)
        logger.info(f"ğŸ“ Fichier d'entrÃ©e: {args.input_file}")
        logger.info(f"ğŸ“ Fichier de sortie: {args.output}")
        
        # VÃ©rification du fichier d'entrÃ©e
        if not os.path.exists(args.input_file):
            logger.error(f"âŒ Fichier non trouvÃ©: {args.input_file}")
            sys.exit(1)
        
        # Ã‰tape 1: Analyse des caractÃ©ristiques audio
        y, sr = librosa.load(args.input_file, sr=16000)
        characteristics = analyze_audio_for_diarization(y, sr, logger)
        
        # Ã‰tape 2: Calcul des paramÃ¨tres adaptatifs
        parameters = calculate_diarization_parameters(characteristics, logger)
        
        # Ã‰tape 3: Diarisation adaptative
        segments = diarize_audio_adaptive(
            args.input_file, 
            parameters, 
            num_workers=args.workers, 
            logger=logger
        )
        
        # Ã‰tape 4: Sauvegarde des rÃ©sultats
        save_diarization_results(segments, args.output, logger)
        
        logger.info("=" * 60)
        logger.info("âœ… DIARISATION TERMINÃ‰E AVEC SUCCÃˆS")
        logger.info("=" * 60)
        logger.info(f"ğŸ“Š {len(segments)} segments identifiÃ©s")
        logger.info(f"ğŸ‘¥ {len(set(s.speaker for s in segments))} locuteurs dÃ©tectÃ©s")
        logger.info(f"ğŸ“ RÃ©sultats: {args.output}")
        
        # Statistiques systÃ¨me
        memory_usage = psutil.Process().memory_info().rss / 1024 / 1024
        logger.info(f"ğŸ“Š MÃ©moire utilisÃ©e: {memory_usage:.1f} MB")
        
    except Exception as e:
        logger.error(f"âŒ Erreur lors de la diarisation: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        sys.exit(1)

if __name__ == "__main__":
    main() 