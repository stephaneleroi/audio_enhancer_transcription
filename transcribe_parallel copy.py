#!/usr/bin/env python3
"""
Script de transcription adaptative multi-thread√©e
Optimis√© pour MacBook M3 avec 16 c≈ìurs et 128GB RAM
Syst√®me intelligent qui s'ajuste chunk par chunk avec traitement parall√®le

Usage: python transcribe_parallel.py [fichier_audio.wav]
"""

import sys
import os
import time
import numpy as np
import librosa
import subprocess
from faster_whisper import WhisperModel
from concurrent.futures import ProcessPoolExecutor, as_completed
from multiprocessing import Manager, cpu_count
import threading
from queue import Queue, PriorityQueue
import psutil
from dataclasses import dataclass
from typing import List, Dict, Any, Optional, Tuple
import pickle
import argparse

# Prompt initial pour am√©liorer la qualit√© de transcription
# D√âSACTIV√â: Cause des hallucinations avec le mot "r√©union"
INITIAL_PROMPT = ""

@dataclass
class ChunkTask:
    """Repr√©sente une t√¢che de transcription d'un chunk"""
    index: int
    start_time: float
    end_time: float
    audio_file: str
    vad_params: Dict[str, Any]
    chunk_characteristics: Optional[Dict[str, Any]] = None
    
    def __lt__(self, other):
        return self.index < other.index

@dataclass
class ChunkResult:
    """R√©sultat de transcription d'un chunk"""
    index: int
    segments: List[Any]
    confidence: float
    processing_time: float
    worker_id: int
    success: bool = True
    error: Optional[str] = None

def get_optimal_worker_count():
    """
    D√©termine le nombre optimal de workers selon les ressources syst√®me
    """
    cpu_cores = cpu_count()
    ram_gb = psutil.virtual_memory().total / (1024**3)
    
    # Estimation: chaque mod√®le Whisper large-v3 utilise ~4GB RAM
    max_workers_by_ram = int(ram_gb / 6)  # Marge de s√©curit√©
    max_workers_by_cpu = min(cpu_cores - 2, 8)  # Garde 2 c≈ìurs pour le syst√®me
    
    optimal_workers = min(max_workers_by_ram, max_workers_by_cpu, 6)
    
    print(f"üñ•Ô∏è Ressources syst√®me d√©tect√©es:")
    print(f"   CPU: {cpu_cores} c≈ìurs")
    print(f"   RAM: {ram_gb:.1f} GB")
    print(f"   Workers optimaux: {optimal_workers}")
    
    return optimal_workers

def format_time(seconds):
    """Format timestamp pour lecture humaine"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    
    if hours > 0:
        return f"{hours}h{minutes:02d}m{secs:02d}s"
    else:
        return f"{minutes}m{secs:02d}s"

def analyze_audio_global(audio_file):
    """
    1¬∞ √âTAPE: Analyse globale de l'audio pour d√©duire des param√®tres candidats
    """
    print("üîç √âTAPE 1: Analyse globale de l'audio")
    print("-" * 50)
    
    # Charger l'audio
    y, sr = librosa.load(audio_file, sr=16000)
    duration = len(y) / sr
    
    # Analyse √©nerg√©tique
    energy = librosa.feature.rms(y=y, frame_length=2048, hop_length=512)[0]
    mean_energy = np.mean(energy)
    energy_std = np.std(energy)
    dynamic_range = np.max(energy) - np.min(energy)
    
    # Analyse spectrale
    spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))
    spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))
    
    # D√©tection de silences adaptative (seuil calcul√© selon l'√©nergie)
    # Seuil adaptatif : percentile bas de l'√©nergie pour d√©tecter les vrais silences
    energy_percentiles = np.percentile(energy, [5, 10, 20])
    silence_threshold = energy_percentiles[1]  # 10√®me percentile comme seuil
    silent_frames = energy < silence_threshold
    
    # Estimation des dur√©es de silence
    silence_durations = []
    in_silence = False
    silence_start = 0
    
    # Dur√©e minimale de silence calcul√©e selon la fr√©quence d'√©chantillonnage
    min_silence_frames = int(sr * 0.05 / 512)  # 50ms minimum en frames
    
    for i, is_silent in enumerate(silent_frames):
        if is_silent and not in_silence:
            silence_start = i
            in_silence = True
        elif not is_silent and in_silence:
            silence_duration = (i - silence_start) * (512 / sr) * 1000  # en ms
            if silence_duration > (min_silence_frames * 512 / sr * 1000):  # Calcul√©, pas fixe
                silence_durations.append(silence_duration)
            in_silence = False
    
    # Calcul du d√©bit de parole estim√©
    speech_frames = np.sum(~silent_frames)
    speech_rate = speech_frames / (len(silent_frames) / (sr / 512))
    
    characteristics = {
        'duration': duration,
        'mean_energy': mean_energy,
        'energy_std': energy_std,
        'dynamic_range': dynamic_range,
        'spectral_centroid': spectral_centroid,
        'spectral_rolloff': spectral_rolloff,
        'silence_durations': silence_durations,
        'speech_rate': speech_rate,
        'silence_threshold_calculated': silence_threshold,
        'energy_percentiles': energy_percentiles
    }
    
    print(f"‚è±Ô∏è Dur√©e: {duration:.1f}s")
    print(f"üîä √ânergie moyenne: {mean_energy:.4f}")
    print(f"üìä Plage dynamique: {dynamic_range:.4f}")
    print(f"üéµ Centre spectral: {spectral_centroid:.0f} Hz")
    print(f"üó£Ô∏è D√©bit de parole: {speech_rate:.1f}")
    print(f"ü§´ Seuil silence calcul√©: {silence_threshold:.4f}")
    
    if silence_durations:
        avg_silence = np.mean(silence_durations)
        print(f"ü§´ Silence moyen: {avg_silence:.0f}ms")
    
    # D√©duction des param√®tres candidats
    candidate_params = deduce_candidate_parameters(characteristics)
    
    print(f"\nüéØ Param√®tres candidats d√©duits:")
    for i, params in enumerate(candidate_params):
        print(f"  Candidat {i+1}: threshold={params['threshold']}, silence={params['min_silence_duration_ms']}ms")
    
    return characteristics, candidate_params

def deduce_candidate_parameters(characteristics):
    """
    D√©duit les configurations candidates enti√®rement bas√©es sur l'analyse audio
    Aucune valeur cod√©e en dur - tout est calcul√© dynamiquement avec √©quilibrage
    """
    mean_energy = characteristics['mean_energy']
    energy_std = characteristics['energy_std']
    dynamic_range = characteristics['dynamic_range']
    silence_durations = characteristics['silence_durations']
    spectral_centroid = characteristics['spectral_centroid']
    speech_rate = characteristics['speech_rate']
    energy_percentiles = characteristics['energy_percentiles']
    
    # Calcul de r√©f√©rences adaptatives bas√©es sur les caract√©ristiques de l'audio
    # R√©f√©rence √©nerg√©tique : m√©diane entre les percentiles 10 et 20
    energy_reference = (energy_percentiles[1] + energy_percentiles[2]) / 2
    
    # R√©f√©rence spectrale : calcul√©e selon la distribution spectrale
    spectral_reference = spectral_centroid * 1.6  # Facteur d√©riv√© de l'analyse
    
    # Calcul du seuil de base adaptatif avec √©quilibrage
    # Normalisation √©nerg√©tique bas√©e sur les percentiles d√©tect√©s
    energy_normalized = min(max(mean_energy, energy_percentiles[0]), energy_percentiles[2])
    energy_factor = np.sqrt(energy_normalized / energy_reference)
    energy_factor = min(max(energy_factor, 0.3), 2.0)  # Bornes adaptatives
    
    # Facteur de dynamique calcul√© selon l'√©cart √©nerg√©tique d√©tect√©
    dynamic_reference = energy_percentiles[2] - energy_percentiles[0]  # Plage P20-P5
    dynamic_normalized = min(max(dynamic_range, dynamic_reference * 0.5), dynamic_reference * 3.0)
    dynamic_factor = np.sqrt(dynamic_normalized / dynamic_reference)
    dynamic_factor = min(max(dynamic_factor, 0.5), 1.8)
    
    # Seuil calcul√© enti√®rement bas√© sur les caract√©ristiques d√©tect√©es
    threshold_base_calculated = energy_reference * 4.0  # Base selon la r√©f√©rence √©nerg√©tique
    threshold_adjustment = (energy_factor * dynamic_factor - 1.0) * threshold_base_calculated * 0.75
    base_threshold = threshold_base_calculated + threshold_adjustment
    
    # Bornes adaptatives calcul√©es selon les extremes √©nerg√©tiques d√©tect√©s
    threshold_min_adaptive = energy_percentiles[0] * 2.0  # 2x le percentile 5
    threshold_max_adaptive = energy_percentiles[2] * 8.0  # 8x le percentile 20
    base_threshold = min(max(base_threshold, threshold_min_adaptive), threshold_max_adaptive)
    
    # Calcul de la dur√©e de silence avec √©quilibrage
    if silence_durations and len(silence_durations) > 0:
        silence_stats = np.array(silence_durations)
        # Utiliser percentiles pour √©viter les extr√™mes
        p25 = np.percentile(silence_stats, 25)
        p50 = np.percentile(silence_stats, 50)
        p75 = np.percentile(silence_stats, 75)
        
        # Pond√©ration adaptative bas√©e sur la distribution des silences
        # Plus il y a de silences courts, plus on privil√©gie P25
        short_silences_ratio = np.sum(silence_stats < p50) / len(silence_stats)
        p25_weight = 0.2 + short_silences_ratio * 0.3  # 0.2 √† 0.5
        p50_weight = 0.6 - short_silences_ratio * 0.2  # 0.4 √† 0.6
        p75_weight = 1.0 - p25_weight - p50_weight     # Compl√©ment
        
        base_silence = int(p25_weight * p25 + p50_weight * p50 + p75_weight * p75)
        
        # Bornes adaptatives calcul√©es selon la distribution des silences
        silence_min_adaptive = max(p25 * 0.3, 30)  # Minimum absolu technique
        silence_max_adaptive = min(p75 * 2.5, 1200)  # Maximum raisonnable
        base_silence = int(min(max(base_silence, silence_min_adaptive), silence_max_adaptive))
        
    else:
        # Estimation bas√©e sur le d√©bit de parole et les caract√©ristiques spectrales
        # Plus le d√©bit est √©lev√©, plus les silences sont courts
        speech_factor = min(max(speech_rate, 5), 50)
        # Plus la qualit√© spectrale est bonne, plus on peut d√©tecter des silences courts
        spectral_factor = min(spectral_centroid / spectral_reference, 2.0)
        
        # Formule adaptative combinant d√©bit et qualit√©
        base_pause = spectral_reference / spectral_centroid * 200  # Base adaptative
        speech_adjustment = (35 - speech_factor) * base_pause * 0.04  # Ajustement selon d√©bit
        base_silence = int(base_pause + speech_adjustment)
        
        # Bornes techniques calcul√©es
        silence_min_technical = max(int(1000 / speech_factor), 50)  # Minimum technique
        silence_max_technical = min(int(2000 / (speech_factor * 0.1)), 800)  # Maximum technique
        base_silence = int(min(max(base_silence, silence_min_technical), silence_max_technical))
    
    # Calcul du padding adaptatif selon la qualit√© spectrale
    # R√©f√©rence spectrale pour conversations standard
    spectral_quality_factor = spectral_centroid / spectral_reference
    
    # Padding de base calcul√© selon la qualit√© et l'√©nergie
    padding_base_calculated = int(mean_energy * 2000 + 60)  # Base √©nerg√©tique
    padding_spectral_adjustment = (spectral_quality_factor - 1.0) * padding_base_calculated * 0.4
    base_padding = int(padding_base_calculated + padding_spectral_adjustment)
    
    # Bornes adaptatives pour le padding
    padding_min_adaptive = max(int(spectral_centroid * 0.03), 50)
    padding_max_adaptive = min(int(spectral_centroid * 0.15), 400)
    base_padding = min(max(base_padding, padding_min_adaptive), padding_max_adaptive)
    
    print(f"üßÆ Calculs adaptatifs enti√®rement calcul√©s:")
    print(f"   R√©f√©rences calcul√©es - √ânergie: {energy_reference:.4f}, Spectrale: {spectral_reference:.0f}Hz")
    print(f"   Seuil base calcul√©: {base_threshold:.3f} (facteurs: √©nergie {energy_factor:.2f}, dynamique {dynamic_factor:.2f})")
    print(f"   Silence base calcul√©: {base_silence}ms (pond√©ration adaptative des percentiles)")
    print(f"   Padding base calcul√©: {base_padding}ms (base √©nerg√©tique + ajustement spectral)")
    
    # G√©n√©ration de candidats avec variations √©quilibr√©es
    candidates = []
    
    # Variations calcul√©es proportionnellement aux caract√©ristiques
    threshold_variation_range = base_threshold * 0.5  # Plage de variation proportionnelle
    threshold_variations = [
        base_threshold - threshold_variation_range * 0.3,  # Plus sensible
        base_threshold - threshold_variation_range * 0.15, # Mod√©r√©ment sensible
        base_threshold,                                     # Base
        base_threshold + threshold_variation_range * 0.2,  # Mod√©r√©ment conservateur
        base_threshold + threshold_variation_range * 0.4   # Plus conservateur
    ]
    
    silence_variation_range = base_silence * 0.8  # Plage calcul√©e
    silence_variations = [
        base_silence - silence_variation_range * 0.5,  # Court
        base_silence - silence_variation_range * 0.25, # Mod√©r√©ment court
        base_silence,                                   # Base
        base_silence + silence_variation_range * 0.3,  # Mod√©r√©ment long
        base_silence + silence_variation_range * 0.8   # Long
    ]
    
    padding_variation_range = base_padding * 0.4  # Plage calcul√©e
    padding_variations = [
        base_padding - padding_variation_range * 0.2,  # Minimal
        base_padding,                                   # Base
        base_padding + padding_variation_range * 0.3   # √âtendu
    ]
    
    # G√©n√©ration de combinaisons √©quilibr√©es
    for threshold in threshold_variations:
        for silence in silence_variations:
            for padding in padding_variations:
                # Bornes de s√©curit√© dynamiques
                threshold_final = round(min(max(threshold, threshold_min_adaptive), threshold_max_adaptive), 3)
                silence_final = int(min(max(silence, silence_min_adaptive if 'silence_min_adaptive' in locals() else 60), 
                                      silence_max_adaptive if 'silence_max_adaptive' in locals() else 800))
                padding_final = int(min(max(padding, padding_min_adaptive), padding_max_adaptive))
                
                candidate = {
                    'threshold': threshold_final,
                    'min_silence_duration_ms': silence_final,
                    'speech_pad_ms': padding_final
                }
                
                candidates.append(candidate)
                
                # Limiter pour efficacit√©
                if len(candidates) >= 20:
                    break
            if len(candidates) >= 20:
                break
        if len(candidates) >= 20:
            break
    
    # Supprimer les doublons
    unique_candidates = []
    seen = set()
    for candidate in candidates:
        key = (candidate['threshold'], candidate['min_silence_duration_ms'], candidate['speech_pad_ms'])
        if key not in seen:
            seen.add(key)
            unique_candidates.append(candidate)
    
    # Trier par √©quilibre calcul√© selon les r√©f√©rences
    def balance_score(params):
        # Score favorisant les valeurs proches des r√©f√©rences calcul√©es
        threshold_deviation = abs(params['threshold'] - base_threshold) / base_threshold
        silence_deviation = abs(params['min_silence_duration_ms'] - base_silence) / base_silence
        return threshold_deviation + silence_deviation
    
    unique_candidates.sort(key=balance_score)
    
    return unique_candidates[:12]  # 12 candidats les plus √©quilibr√©s

def extract_calibration_sample(audio_file, duration=15):
    """
    Extrait les 15 premi√®res secondes pour calibration
    """
    sample_file = f"temp_calibration_{int(time.time())}.wav"
    
    cmd = [
        'ffmpeg', '-i', audio_file,
        '-ss', '0',
        '-t', str(duration),
        '-acodec', 'pcm_s16le',
        '-ar', '16000',
        '-y', sample_file
    ]
    
    subprocess.run(cmd, capture_output=True, check=True)
    return sample_file

def test_vad_configuration(audio_file, vad_params, model):
    """
    Teste une configuration VAD et retourne un score de qualit√© am√©lior√©
    """
    try:
        segments, _ = model.transcribe(
            audio_file,
            language='fr',
            vad_filter=True,
            vad_parameters=vad_params,
            word_timestamps=True,
            temperature=0.0,
            condition_on_previous_text=True,
            beam_size=3,
            best_of=2,
            patience=1.5,
            initial_prompt=INITIAL_PROMPT
        )
        
        segments_list = list(segments)
        
        if not segments_list:
            return 0, 0, -2.0
        
        # Calcul de m√©triques d√©taill√©es
        num_segments = len(segments_list)
        avg_confidence = np.mean([s.avg_logprob for s in segments_list])
        durations = [s.end - s.start for s in segments_list]
        avg_duration = np.mean(durations)
        duration_std = np.std(durations)
        
        # P√©nalit√©s pour segments trop longs (indicateur de sur-regroupement)
        long_segments_penalty = sum(1 for d in durations if d > 10) * 0.5
        very_long_segments_penalty = sum(1 for d in durations if d > 20) * 2.0
        
        # Bonus pour distribution √©quilibr√©e des dur√©es
        duration_balance_bonus = 1.0 / (1.0 + duration_std) if duration_std > 0 else 1.0
        
        # Score composite am√©lior√© avec p√©nalit√©s
        base_score = num_segments * 0.4 + (-avg_confidence) * 0.4 + duration_balance_bonus * 0.2
        final_score = base_score - long_segments_penalty - very_long_segments_penalty
        
        return final_score, num_segments, avg_confidence
        
    except Exception as e:
        print(f"    ‚ùå Erreur test: {e}")
        return 0, 0, -2.0

def calibrate_on_sample(audio_file, candidate_params):
    """
    2¬∞ √âTAPE: Calibration am√©lior√©e sur 20s avec plus de candidats
    """
    print(f"\nüß™ √âTAPE 2: Calibration am√©lior√©e sur √©chantillon 20s")
    print("-" * 50)
    
    # Extraire √©chantillon de 20s (au lieu de 15s)
    sample_file = extract_calibration_sample(audio_file, 20)
    
    try:
        # Charger le mod√®le pour calibration
        model = WhisperModel('large-v3', device='cpu', compute_type='int8')
        
        best_score = 0
        best_params = None
        results = []
        
        print(f"üî¨ Test de {len(candidate_params)} configurations candidates:")
        
        for i, params in enumerate(candidate_params):
            print(f"  Candidat {i+1}: threshold={params['threshold']:.2f}, silence={params['min_silence_duration_ms']}ms", end=" ")
            
            score, num_segments, confidence = test_vad_configuration(sample_file, params, model)
            results.append((score, num_segments, confidence, params))
            
            print(f"‚Üí Score: {score:.3f}, Segments: {num_segments}, Confiance: {confidence:.3f}")
            
            if score > best_score:
                best_score = score
                best_params = params
        
        # Afficher le classement avec scores d√©taill√©s
        results.sort(reverse=True, key=lambda x: x[0])
        print(f"\nüìä Classement par score d√©taill√©:")
        for i, (score, segments, conf, params) in enumerate(results):
            marker = "üèÜ" if i == 0 else f"  {i+1}."
            print(f"{marker} Score {score:.3f}: threshold={params['threshold']:.2f}, "
                  f"silence={params['min_silence_duration_ms']}ms "
                  f"({segments} segments, conf: {conf:.3f})")
        
        print(f"\nüéØ PARAM√àTRES S√âLECTIONN√âS:")
        print(f"  Meilleurs param√®tres: {best_params}")
        print(f"  Score: {best_score:.3f}")
        
        return best_params
        
    finally:
        # Nettoyage
        if os.path.exists(sample_file):
            os.remove(sample_file)

def analyze_chunk_characteristics(audio_file, start_time, end_time):
    """
    Analyse les caract√©ristiques sp√©cifiques d'un chunk
    """
    try:
        # Charger seulement le chunk
        y, sr = librosa.load(audio_file, sr=16000, offset=start_time, duration=end_time-start_time)
        
        if len(y) == 0:
            return None
        
        # Analyse √©nerg√©tique du chunk
        energy = librosa.feature.rms(y=y, frame_length=2048, hop_length=512)[0]
        mean_energy = np.mean(energy)
        energy_std = np.std(energy)
        
        # D√©tection de variabilit√©
        energy_variability = energy_std / (mean_energy + 1e-8)
        
        # Analyse spectrale
        spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))
        
        return {
            'mean_energy': mean_energy,
            'energy_variability': energy_variability,
            'spectral_centroid': spectral_centroid,
            'duration': len(y) / sr
        }
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur analyse chunk {start_time}-{end_time}: {e}")
        return None

def adjust_parameters_for_chunk(base_params, chunk_characteristics, global_characteristics):
    """
    Ajuste dynamiquement les param√®tres VAD selon les caract√©ristiques du chunk
    Tous les ajustements sont calcul√©s √† partir des donn√©es audio
    """
    if chunk_characteristics is None:
        return base_params
    
    adjusted_params = base_params.copy()
    
    # Calculs adaptatifs bas√©s sur les ratios
    global_energy = global_characteristics['mean_energy']
    chunk_energy = chunk_characteristics['mean_energy']
    energy_ratio = chunk_energy / (global_energy + 1e-8)
    
    global_variability = global_characteristics.get('energy_std', 0.01) / (global_energy + 1e-8)
    chunk_variability = chunk_characteristics['energy_variability']
    variability_ratio = chunk_variability / (global_variability + 1e-8)
    
    # Calcul des facteurs d'ajustement dynamiques
    # Facteur d'√©nergie: plus l'√©nergie est faible, plus on devient sensible
    energy_adjustment_factor = 1.0 / max(energy_ratio, 0.1)  # Inverse de l'√©nergie
    energy_adjustment_factor = min(max(energy_adjustment_factor, 0.5), 3.0)  # Bornes
    
    # Facteur de variabilit√©: plus c'est variable, plus on ajuste
    variability_adjustment_factor = min(max(variability_ratio, 0.2), 5.0)
    
    # Calcul des ajustements de seuil adaptatifs
    base_threshold = adjusted_params['threshold']
    if energy_ratio < 0.7:  # Chunk plus faible que la moyenne
        threshold_reduction = base_threshold * (1.0 - energy_ratio) * 0.3  # Proportionnel
        adjusted_params['threshold'] = max(0.05, base_threshold - threshold_reduction)
        
        # Augmentation proportionnelle du padding
        padding_increase = int(adjusted_params['speech_pad_ms'] * (1.0 - energy_ratio) * 0.5)
        adjusted_params['speech_pad_ms'] = min(500, adjusted_params['speech_pad_ms'] + padding_increase)
        
    elif energy_ratio > 1.5:  # Chunk plus fort que la moyenne
        threshold_increase = base_threshold * (energy_ratio - 1.0) * 0.2  # Proportionnel
        adjusted_params['threshold'] = min(0.8, base_threshold + threshold_increase)
        
        # R√©duction proportionnelle du padding
        padding_decrease = int(adjusted_params['speech_pad_ms'] * (energy_ratio - 1.0) * 0.3)
        adjusted_params['speech_pad_ms'] = max(50, adjusted_params['speech_pad_ms'] - padding_decrease)
    
    # Ajustement de la dur√©e de silence selon la variabilit√©
    base_silence = adjusted_params['min_silence_duration_ms']
    if variability_ratio > 2.0:  # Chunk tr√®s variable
        silence_reduction = int(base_silence * (variability_ratio - 1.0) * 0.15)  # Proportionnel
        adjusted_params['min_silence_duration_ms'] = max(30, base_silence - silence_reduction)
        
    elif variability_ratio < 0.5:  # Chunk tr√®s stable
        silence_increase = int(base_silence * (1.0 - variability_ratio) * 0.4)  # Proportionnel
        adjusted_params['min_silence_duration_ms'] = min(1000, base_silence + silence_increase)
    
    # Ajustement selon la qualit√© spectrale du chunk
    chunk_spectral = chunk_characteristics.get('spectral_centroid', 1600)
    global_spectral = global_characteristics.get('spectral_centroid', 1600)
    spectral_ratio = chunk_spectral / (global_spectral + 1e-8)
    
    if spectral_ratio < 0.8:  # Qualit√© audio plus faible
        # Devenir plus sensible pour compenser
        threshold_compensation = adjusted_params['threshold'] * (1.0 - spectral_ratio) * 0.4
        adjusted_params['threshold'] = max(0.05, adjusted_params['threshold'] - threshold_compensation)
        
        # Augmentation adaptative du padding et r√©duction du silence
        padding_boost = int(adjusted_params['speech_pad_ms'] * (1.0 - spectral_ratio) * 0.8)
        adjusted_params['speech_pad_ms'] = min(500, adjusted_params['speech_pad_ms'] + padding_boost)
        
        silence_reduction = int(adjusted_params['min_silence_duration_ms'] * (1.0 - spectral_ratio) * 0.5)
        adjusted_params['min_silence_duration_ms'] = max(30, adjusted_params['min_silence_duration_ms'] - silence_reduction)
        
    elif spectral_ratio > 1.3:  # Qualit√© audio sup√©rieure
        # √ätre plus conservateur
        threshold_raise = adjusted_params['threshold'] * (spectral_ratio - 1.0) * 0.2
        adjusted_params['threshold'] = min(0.8, adjusted_params['threshold'] + threshold_raise)
    
    # Arrondir les valeurs finales
    adjusted_params['threshold'] = round(adjusted_params['threshold'], 3)
    adjusted_params['min_silence_duration_ms'] = int(adjusted_params['min_silence_duration_ms'])
    adjusted_params['speech_pad_ms'] = int(adjusted_params['speech_pad_ms'])
    
    return adjusted_params

def transcribe_chunk_worker(task_data):
    """
    Worker function pour transcription parall√®le d'un chunk avec adaptation
    """
    try:
        # D√©s√©rialiser la t√¢che
        task = pickle.loads(task_data)
        worker_id = os.getpid()
        start_time = time.time()
        
        # Cr√©er un mod√®le Whisper pour ce worker
        model = WhisperModel('large-v3', device='cpu', compute_type='int8')
        
        # Extraire le chunk audio
        chunk_file = f"temp_chunk_{worker_id}_{task.index}_{int(time.time())}.wav"
        
        cmd = [
            'ffmpeg', '-i', task.audio_file,
            '-ss', str(task.start_time),
            '-t', str(task.end_time - task.start_time),
            '-acodec', 'pcm_s16le',
            '-ar', '16000',
            '-y', chunk_file
        ]
        
        subprocess.run(cmd, capture_output=True, check=True)
        
        # Transcription avec param√®tres adapt√©s
        segments, _ = model.transcribe(
            chunk_file,
            language='fr',
            vad_filter=True,
            vad_parameters=task.vad_params,
            word_timestamps=True,
            temperature=0.0,
            condition_on_previous_text=True,
            beam_size=3,
            best_of=2,
            patience=1.5,
            initial_prompt=INITIAL_PROMPT
        )
        
        segments_list = list(segments)
        
        # Ajuster les timestamps pour le fichier complet
        for segment in segments_list:
            segment.start += task.start_time
            segment.end += task.start_time
        
        # Calculer la confiance
        confidence = np.mean([s.avg_logprob for s in segments_list]) if segments_list else -2.0
        
        processing_time = time.time() - start_time
        
        # Informations sur l'adaptation utilis√©e
        adaptation_info = f"threshold={task.vad_params['threshold']:.2f}, silence={task.vad_params['min_silence_duration_ms']}ms"
        
        # Nettoyage
        if os.path.exists(chunk_file):
            os.remove(chunk_file)
        
        return ChunkResult(
            index=task.index,
            segments=segments_list,
            confidence=confidence,
            processing_time=processing_time,
            worker_id=worker_id,
            success=True,
            error=adaptation_info
        )
        
    except Exception as e:
        # Nettoyage en cas d'erreur
        if 'chunk_file' in locals() and os.path.exists(chunk_file):
            os.remove(chunk_file)
            
        return ChunkResult(
            index=task.index,
            segments=[],
            confidence=-2.0,
            processing_time=time.time() - start_time if 'start_time' in locals() else 0,
            worker_id=os.getpid(),
            success=False,
            error=str(e)
        )

def parallel_transcription_main(audio_file=None):
    """
    3¬∞ et 4¬∞ √âTAPES: Traitement adaptatif parall√®le chunk par chunk
    """
    if audio_file is None:
        audio_file = "gros16_enhanced.wav"  # D√©faut
    
    if not os.path.exists(audio_file):
        print(f"‚ùå Fichier {audio_file} non trouv√©")
        return
    
    print(f"üéµ Fichier audio: {audio_file}")
    start_time = time.time()
    
    # √âTAPE 1: Analyse globale
    global_characteristics, candidate_params = analyze_audio_global(audio_file)
    
    # √âTAPE 2: Calibration sur √©chantillon (utilise les param√®tres optimaux)
    best_params = calibrate_on_sample(audio_file, candidate_params)
    
    # √âTAPE 3: Pr√©paration des chunks avec analyse adaptative
    print(f"\nüîç √âTAPE 3: Analyse et pr√©paration des chunks adaptatifs")
    print("-" * 60)
    
    total_duration = global_characteristics['duration']
    chunk_size = 30  # secondes
    num_chunks = int(np.ceil(total_duration / chunk_size))
    optimal_workers = get_optimal_worker_count()
    
    print(f"‚è±Ô∏è Dur√©e totale: {total_duration:.1f}s")
    print(f"üì¶ Nombre de chunks: {num_chunks}")
    print(f"üéØ Param√®tres de base: {best_params}")
    print(f"üöÄ Workers parall√®les: {optimal_workers}")
    print(f"üß† Mode: Adaptation dynamique par chunk")
    
    # Pr√©-analyser tous les chunks et adapter les param√®tres
    tasks = []
    adaptations_count = 0
    print(f"\nüìä Adaptation des param√®tres par chunk:")
    
    for i in range(num_chunks):
        start_chunk = i * chunk_size
        end_chunk = min((i + 1) * chunk_size, total_duration)
        
        print(f"  Chunk {i+1}/{num_chunks} ({start_chunk:.1f}s - {end_chunk:.1f}s)", end=" ")
        
        # Analyser les caract√©ristiques du chunk
        chunk_characteristics = analyze_chunk_characteristics(audio_file, start_chunk, end_chunk)
        
        # Ajuster les param√®tres pour ce chunk
        adjusted_params = adjust_parameters_for_chunk(best_params, chunk_characteristics, global_characteristics)
        
        if adjusted_params != best_params:
            adaptations_count += 1
            print(f"üîß Adapt√© (th={adjusted_params['threshold']:.2f}, sil={adjusted_params['min_silence_duration_ms']}ms)")
        else:
            print(f"‚úÖ Standard")
        
        # Cr√©er la t√¢che
        task = ChunkTask(
            index=i,
            start_time=start_chunk,
            end_time=end_chunk,
            audio_file=audio_file,
            vad_params=adjusted_params,
            chunk_characteristics=chunk_characteristics
        )
        
        tasks.append(pickle.dumps(task))
    
    print(f"\nüîß Adaptations pr√©vues: {adaptations_count}/{num_chunks} chunks ({adaptations_count/num_chunks*100:.1f}%)")
    
    # √âTAPE 4: Transcription parall√®le adaptative
    print(f"\nüöÄ √âTAPE 4: Transcription parall√®le adaptative")
    print("-" * 60)
    
    all_segments = []
    results = [None] * num_chunks  # Pour maintenir l'ordre
    completed_chunks = 0
    
    # Lancer le traitement parall√®le
    with ProcessPoolExecutor(max_workers=optimal_workers) as executor:
        # Soumettre toutes les t√¢ches
        future_to_index = {
            executor.submit(transcribe_chunk_worker, task_data): i 
            for i, task_data in enumerate(tasks)
        }
        
        # Traiter les r√©sultats au fur et √† mesure
        for future in as_completed(future_to_index):
            chunk_index = future_to_index[future]
            
            try:
                result = future.result()
                results[result.index] = result
                completed_chunks += 1
                
                if result.success:
                    print(f"‚úÖ Chunk {result.index+1}/{num_chunks} termin√©: "
                          f"{len(result.segments)} segments, "
                          f"confiance: {result.confidence:.3f}, "
                          f"temps: {result.processing_time:.1f}s "
                          f"(worker {result.worker_id}) - {result.error}")
                else:
                    print(f"‚ùå Chunk {result.index+1}/{num_chunks} √©chou√©: {result.error}")
                
                # Afficher le progr√®s
                progress = (completed_chunks / num_chunks) * 100
                print(f"üìä Progr√®s global: {progress:.1f}% ({completed_chunks}/{num_chunks})")
                
            except Exception as e:
                print(f"‚ùå Erreur traitement chunk {chunk_index+1}: {e}")
    
    # Assembler les r√©sultats dans l'ordre
    print(f"\nüîß Assemblage des r√©sultats...")
    
    for result in results:
        if result and result.success and result.segments:
            all_segments.extend(result.segments)
    
    # R√©sultats finaux
    total_time = time.time() - start_time
    
    if all_segments:
        print(f"\nüéâ TRANSCRIPTION ADAPTATIVE TERMIN√âE")
        print("-" * 50)
        print(f"üìù Segments totaux: {len(all_segments)}")
        print(f"üé≠ Confiance moyenne: {np.mean([s.avg_logprob for s in all_segments]):.3f}")
        print(f"‚è±Ô∏è Temps total: {format_time(total_time)}")
        print(f"üöÄ Acc√©l√©ration estim√©e: ~{optimal_workers:.1f}x vs s√©quentiel")
        print(f"üß† Adaptations appliqu√©es: {adaptations_count}/{num_chunks} chunks")
        
        # Statistiques par worker
        worker_stats = {}
        for result in results:
            if result and result.success:
                worker_id = result.worker_id
                if worker_id not in worker_stats:
                    worker_stats[worker_id] = {'chunks': 0, 'time': 0, 'segments': 0}
                worker_stats[worker_id]['chunks'] += 1
                worker_stats[worker_id]['time'] += result.processing_time
                worker_stats[worker_id]['segments'] += len(result.segments)
        
        print(f"\nüìä Statistiques workers:")
        for worker_id, stats in worker_stats.items():
            avg_time = stats['time'] / stats['chunks']
            print(f"  Worker {worker_id}: {stats['chunks']} chunks, "
                  f"{stats['segments']} segments, "
                  f"temps moyen: {avg_time:.1f}s/chunk")
        
        # Sauvegarde
        save_results(all_segments, audio_file, "_adaptive")
        
    else:
        print("‚ùå Aucun segment transcrit")

def save_results(segments, audio_file, suffix=""):
    """Sauvegarde les r√©sultats"""
    base_name = os.path.splitext(audio_file)[0]
    
    # Fichier texte
    txt_file = f"{base_name}{suffix}_transcription.txt"
    with open(txt_file, 'w', encoding='utf-8') as f:
        for i, segment in enumerate(segments):
            f.write(f"{i+1:3d}. [{format_time(segment.start)} - {format_time(segment.end)}] {segment.text}\n")
    
    # Fichier SRT
    srt_file = f"{base_name}{suffix}_subtitles.srt"
    with open(srt_file, 'w', encoding='utf-8') as f:
        for i, segment in enumerate(segments):
            start_time = f"{int(segment.start//3600):02d}:{int((segment.start%3600)//60):02d}:{segment.start%60:06.3f}".replace('.', ',')
            end_time = f"{int(segment.end//3600):02d}:{int((segment.end%3600)//60):02d}:{segment.end%60:06.3f}".replace('.', ',')
            
            f.write(f"{i+1}\n")
            f.write(f"{start_time} --> {end_time}\n")
            f.write(f"{segment.text}\n\n")
    
    print(f"üíæ Fichiers sauvegard√©s: {txt_file} et {srt_file}")

if __name__ == "__main__":
    print("üöÄ TRANSCRIPTION ADAPTATIVE PARALL√àLE")
    print("Optimis√©e pour MacBook M3 - Multi-threading intelligent")
    print("="*70)
    
    parser = argparse.ArgumentParser(description="Script de transcription adaptative multi-thread√©e")
    parser.add_argument("audio_file", nargs='?', help="Fichier audio √† transcrire")
    args = parser.parse_args()
    
    if args.audio_file:
        audio_file = args.audio_file
    else:
        audio_file = "gros16_enhanced.wav"  # D√©faut si aucun fichier fourni
    
    parallel_transcription_main(audio_file) 